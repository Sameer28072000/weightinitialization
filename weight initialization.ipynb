{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff269f95-31e4-4626-86f0-5ce770eef0c7",
   "metadata": {},
   "source": [
    "Part 1:Understanding weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc6a93b-9ec9-48bc-aef4-5eadf407104c",
   "metadata": {},
   "source": [
    "Q1.Explain the importance of weight initialization in artificial neural networks. Why is it necessary to initialize\n",
    "the weights carefully?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055dec1-8ad8-40a5-b8ec-0d0adec32025",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-Weight initialization plays a crucial role in training artificial neural networks.\n",
    "      i.Breaking symmetry\n",
    "        ii.Preventing vanishing\n",
    "          iii.Avoiding local minima\n",
    "            iv.Improving generaling\n",
    "            \n",
    "        These are the techiniques of weight initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481f0c9-60c6-4671-98bb-b79e74a08857",
   "metadata": {},
   "source": [
    "Q2.Describe the challenges associated with improper weight initialization. How do these issues affect model\n",
    "training and convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5ae667-8ac5-46f6-96e2-90d58f069e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-Improper weight initialization in artificial neural networks can lead to\n",
    "       several challenges during model training and convergence:\n",
    "        i.Vanishing gradients\n",
    "          ii.Slow convergence\n",
    "            iii.Gradient instability\n",
    "              iv.Overfitting\n",
    "                \n",
    "    Proper weight initialization techniques aim to mitigate these challenges \n",
    "      and facilitate successful training and convergence of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a5c3a-1c00-4f7c-aef0-be3ec97c83b4",
   "metadata": {},
   "source": [
    "Q3.Discuss the concept of variance and how it relates to weight initialization. Why is it crucial to consider the\n",
    "variance of weights during initialization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb3de9-3b40-4cb9-bbca-48a98eebb8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-Variance is a statistical measure that quantifies the spread or dispersion of a set of values.\n",
    "      The variance of weights during initialization is crucial for several reasons:\n",
    "        i.Activation output variance\n",
    "          ii.Gradient variance\n",
    "            iii.Initialization bias\n",
    "    Considering the variance of weights during initialization is crucial to ensure stable \n",
    "      and effective training of neural networks.\n",
    "        \n",
    "        The network can start with a reasonable range of weights that allow for efficient learning and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e15de-4a41-40b1-888c-029b7fbc13ad",
   "metadata": {},
   "source": [
    "Q4.Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate\n",
    "to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8682e8-8baa-4943-a8de-bade3b1e5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-Zero initialization is a weight initialization technique where all the weights in a neural network are set to zero.\n",
    "       When all weights are initialized to zero, it can lead to several challenges:\n",
    "          i.Symmetry breaking\n",
    "            ii.Identical neauron behaviour\n",
    "              iii.zero gradients\n",
    "                \n",
    "        Despite its limitations, zero initialization can be appropriate in certain cases:\n",
    "            i.Bias initialization \n",
    "              ii.Transfer learning\n",
    "                \n",
    "            zero initialization is not widely used for weight initialization\n",
    "                in the hidden layers of neural networks because of its limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1695e57-5ca3-446f-bfba-0dcc13078d0e",
   "metadata": {},
   "source": [
    "Q5.Describe the process of random initialization. How can random initialization be adjusted to mitigate\n",
    "potential issues like saturation or vanishing/exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe89250-568a-4003-84db-4eb8fb628587",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-Random initialization is a weight initialization technique where \n",
    "       the weights in a neural network are initialized with random values. \n",
    "    \n",
    "          The process of random initialization involves the following steps:\n",
    "            i.Selecting a porbability distribution\n",
    "              ii.Adjusting the variance\n",
    "                \n",
    "        To mitigate saturation issues, which occur when the activations of neurons become\n",
    "            too close to the extreme values (e.g., 0 or 1 for sigmoid activation).\n",
    "           \n",
    "            To address the problem of vanishing/exploding gradients,\n",
    "                which occur when the gradients become either very small or very large.\n",
    "          \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af8aed0-3e3a-4884-b25f-208f8354a623",
   "metadata": {},
   "source": [
    "Q6.Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper\n",
    "weight initialization and the underlEing theorE behind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd3ab9-3808-4841-a1de-b0fce0484eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-\n",
    "     Xavier (also known as Glorot) initialization is a popular weight initialization technique \n",
    "        that aims to address the challenges associated with improper weight initialization\n",
    "           such as the vanishing and exploding gradients problems.\n",
    "            \n",
    "        Xavier initialization provides a principled approach to weight initialization in neural networks, \n",
    "            addressing the challenges ofimproper initialization and promoting stable and efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9736b303-f0d2-42a8-9bc7-d811d3462201",
   "metadata": {},
   "source": [
    "Q7.Explain the concept of He initialization. How does it differ from Xavier initialization, and when is it\n",
    "preferred?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961f7bc-da8a-4e85-a83d-d91b020e26ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-He initialization is another commonly used weight initialization technique that addresses \n",
    "        the challenges of improper weight initialization, particularly for networks that use \n",
    "            activation functionslike ReLU (Rectified Linear Unit) and its variants.\n",
    "        \n",
    "        Compared to Xavier initialization, the key difference in He initialization lies in the variance calculation. \n",
    "        \n",
    "           He initialization uses a factor of 2 divided by the number of input connections \n",
    "            whereas Xavier initialization uses the sum of input & output connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab583b8d-bd99-4294-86cf-0b13ba47405e",
   "metadata": {},
   "source": [
    "Q8. Implement different weight initialization techniques (zero initialization, random initialization, Xavier\n",
    "initialization, and He initialization) in a neural network using a framework of your choice. Train the model\n",
    "on a suitable dataset and compare the performance of the initialized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de8a28b-7fa5-4698-93f7-21f8f7e380ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "546/546 [==============================] - 4s 6ms/step - loss: 2.3021 - accuracy: 0.1121 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
      "Epoch 2/10\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 3/10\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 4/10\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 5/10\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/10\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/10\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/10\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/10\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/10\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18e8265930>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784) / 255.0\n",
    "x_test = x_test.reshape(-1, 784) / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(784,), kernel_initializer='zeros'))\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='random_uniform'))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=SGD(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=110, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117dcb10-f3a8-4e79-b5e9-fbffa3aaaed5",
   "metadata": {},
   "source": [
    "Q9.Discuss the considerations and tradeoffs when choosing the appropriate weight initialization technique\n",
    "for a given neural network architecture and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025de10-7f70-424f-bb61-40e0936fad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:-\n",
    "      When choosing the appropriate weight initialization technique for a neural network, \n",
    "           there are several considerations and tradeoffs to keep in mind. \n",
    "        \n",
    "        Some key consideration:-\n",
    "          i.Activation function\n",
    "            ii.Network depth\n",
    "              iii.Network architecture\n",
    "                iv.Data characterstics\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
